{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import joblib\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting 1 : A->R  2: C->R\n",
    "# Load the datasets\n",
    "A_A = pd.read_csv('./Office-Home_resnet50/Art_Art.csv', header=None)\n",
    "A_R = pd.read_csv('./Office-Home_resnet50/Art_RealWorld.csv', header=None)\n",
    "\n",
    "# Assuming the last column is the label\n",
    "Y_A = A_A.iloc[:, -1]\n",
    "Y_R = A_R.iloc[:, -1]\n",
    "\n",
    "X_A = A_A.iloc[:, :-1]\n",
    "X_R = A_R.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.177738</td>\n",
       "      <td>0.471652</td>\n",
       "      <td>0.541455</td>\n",
       "      <td>0.079318</td>\n",
       "      <td>0.367973</td>\n",
       "      <td>0.247756</td>\n",
       "      <td>1.005723</td>\n",
       "      <td>0.101805</td>\n",
       "      <td>2.486836</td>\n",
       "      <td>0.188542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124564</td>\n",
       "      <td>0.575772</td>\n",
       "      <td>0.451677</td>\n",
       "      <td>0.503198</td>\n",
       "      <td>0.408284</td>\n",
       "      <td>0.370589</td>\n",
       "      <td>0.138303</td>\n",
       "      <td>1.269258</td>\n",
       "      <td>0.714142</td>\n",
       "      <td>0.119987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.224926</td>\n",
       "      <td>0.200609</td>\n",
       "      <td>0.067538</td>\n",
       "      <td>1.029995</td>\n",
       "      <td>0.273691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.354926</td>\n",
       "      <td>0.840569</td>\n",
       "      <td>0.095307</td>\n",
       "      <td>0.027296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365344</td>\n",
       "      <td>0.198707</td>\n",
       "      <td>1.399693</td>\n",
       "      <td>0.603393</td>\n",
       "      <td>1.222108</td>\n",
       "      <td>0.172571</td>\n",
       "      <td>0.954101</td>\n",
       "      <td>0.265650</td>\n",
       "      <td>0.282168</td>\n",
       "      <td>0.026532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.082612</td>\n",
       "      <td>0.405891</td>\n",
       "      <td>0.090812</td>\n",
       "      <td>0.408200</td>\n",
       "      <td>0.256042</td>\n",
       "      <td>0.110931</td>\n",
       "      <td>0.530202</td>\n",
       "      <td>0.895347</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.398198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066899</td>\n",
       "      <td>0.176659</td>\n",
       "      <td>0.359395</td>\n",
       "      <td>0.377675</td>\n",
       "      <td>0.091454</td>\n",
       "      <td>0.402437</td>\n",
       "      <td>0.138679</td>\n",
       "      <td>0.673525</td>\n",
       "      <td>0.088165</td>\n",
       "      <td>0.081927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.670875</td>\n",
       "      <td>1.358731</td>\n",
       "      <td>0.108823</td>\n",
       "      <td>0.433988</td>\n",
       "      <td>0.751752</td>\n",
       "      <td>0.255669</td>\n",
       "      <td>0.828045</td>\n",
       "      <td>0.248858</td>\n",
       "      <td>1.179098</td>\n",
       "      <td>0.175501</td>\n",
       "      <td>...</td>\n",
       "      <td>1.592119</td>\n",
       "      <td>0.726908</td>\n",
       "      <td>0.105380</td>\n",
       "      <td>0.464521</td>\n",
       "      <td>0.435159</td>\n",
       "      <td>0.291601</td>\n",
       "      <td>0.380927</td>\n",
       "      <td>0.423609</td>\n",
       "      <td>0.114927</td>\n",
       "      <td>0.189419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.869736</td>\n",
       "      <td>0.729677</td>\n",
       "      <td>0.028039</td>\n",
       "      <td>0.016968</td>\n",
       "      <td>3.375509</td>\n",
       "      <td>0.022381</td>\n",
       "      <td>1.716877</td>\n",
       "      <td>0.134307</td>\n",
       "      <td>0.566226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197924</td>\n",
       "      <td>0.048724</td>\n",
       "      <td>0.188837</td>\n",
       "      <td>0.223754</td>\n",
       "      <td>0.042908</td>\n",
       "      <td>0.075234</td>\n",
       "      <td>0.171219</td>\n",
       "      <td>0.348125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4352</th>\n",
       "      <td>0.045199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094151</td>\n",
       "      <td>1.798568</td>\n",
       "      <td>1.532259</td>\n",
       "      <td>0.074363</td>\n",
       "      <td>0.586506</td>\n",
       "      <td>0.790157</td>\n",
       "      <td>0.523200</td>\n",
       "      <td>0.052590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032262</td>\n",
       "      <td>1.513051</td>\n",
       "      <td>0.404489</td>\n",
       "      <td>1.358293</td>\n",
       "      <td>0.007750</td>\n",
       "      <td>1.634978</td>\n",
       "      <td>0.037231</td>\n",
       "      <td>0.249093</td>\n",
       "      <td>0.293531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4353</th>\n",
       "      <td>0.336516</td>\n",
       "      <td>0.511065</td>\n",
       "      <td>1.238201</td>\n",
       "      <td>0.237525</td>\n",
       "      <td>0.921986</td>\n",
       "      <td>0.567462</td>\n",
       "      <td>0.231861</td>\n",
       "      <td>0.648776</td>\n",
       "      <td>0.284473</td>\n",
       "      <td>0.119037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048069</td>\n",
       "      <td>0.175962</td>\n",
       "      <td>0.443320</td>\n",
       "      <td>0.247849</td>\n",
       "      <td>0.079690</td>\n",
       "      <td>0.621297</td>\n",
       "      <td>0.088509</td>\n",
       "      <td>0.536190</td>\n",
       "      <td>0.017956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4354</th>\n",
       "      <td>0.077790</td>\n",
       "      <td>0.091784</td>\n",
       "      <td>0.031461</td>\n",
       "      <td>0.467954</td>\n",
       "      <td>0.042052</td>\n",
       "      <td>0.293284</td>\n",
       "      <td>0.528290</td>\n",
       "      <td>0.369393</td>\n",
       "      <td>0.303122</td>\n",
       "      <td>0.133160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241998</td>\n",
       "      <td>0.342492</td>\n",
       "      <td>1.211094</td>\n",
       "      <td>0.260014</td>\n",
       "      <td>0.319898</td>\n",
       "      <td>0.348717</td>\n",
       "      <td>0.205331</td>\n",
       "      <td>0.615079</td>\n",
       "      <td>0.037420</td>\n",
       "      <td>0.101339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4355</th>\n",
       "      <td>0.641730</td>\n",
       "      <td>0.073805</td>\n",
       "      <td>0.219991</td>\n",
       "      <td>0.344924</td>\n",
       "      <td>0.334943</td>\n",
       "      <td>0.145965</td>\n",
       "      <td>0.754356</td>\n",
       "      <td>0.881679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.121588</td>\n",
       "      <td>0.042624</td>\n",
       "      <td>0.299990</td>\n",
       "      <td>0.657176</td>\n",
       "      <td>0.152649</td>\n",
       "      <td>0.199864</td>\n",
       "      <td>0.216885</td>\n",
       "      <td>0.111537</td>\n",
       "      <td>0.039340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4356</th>\n",
       "      <td>0.154358</td>\n",
       "      <td>1.466534</td>\n",
       "      <td>0.090719</td>\n",
       "      <td>0.074424</td>\n",
       "      <td>0.494054</td>\n",
       "      <td>0.461551</td>\n",
       "      <td>0.063810</td>\n",
       "      <td>0.116978</td>\n",
       "      <td>0.041561</td>\n",
       "      <td>0.188517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137291</td>\n",
       "      <td>0.067129</td>\n",
       "      <td>0.042314</td>\n",
       "      <td>0.839931</td>\n",
       "      <td>0.048468</td>\n",
       "      <td>0.374847</td>\n",
       "      <td>0.817358</td>\n",
       "      <td>0.125033</td>\n",
       "      <td>0.029065</td>\n",
       "      <td>0.532317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4357 rows × 2048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     1.177738  0.471652  0.541455  0.079318  0.367973  0.247756  1.005723   \n",
       "1     0.224926  0.200609  0.067538  1.029995  0.273691  0.000000  1.354926   \n",
       "2     0.082612  0.405891  0.090812  0.408200  0.256042  0.110931  0.530202   \n",
       "3     0.670875  1.358731  0.108823  0.433988  0.751752  0.255669  0.828045   \n",
       "4     0.869736  0.729677  0.028039  0.016968  3.375509  0.022381  1.716877   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4352  0.045199  0.000000  0.094151  1.798568  1.532259  0.074363  0.586506   \n",
       "4353  0.336516  0.511065  1.238201  0.237525  0.921986  0.567462  0.231861   \n",
       "4354  0.077790  0.091784  0.031461  0.467954  0.042052  0.293284  0.528290   \n",
       "4355  0.641730  0.073805  0.219991  0.344924  0.334943  0.145965  0.754356   \n",
       "4356  0.154358  1.466534  0.090719  0.074424  0.494054  0.461551  0.063810   \n",
       "\n",
       "          7         8         9     ...      2038      2039      2040  \\\n",
       "0     0.101805  2.486836  0.188542  ...  0.124564  0.575772  0.451677   \n",
       "1     0.840569  0.095307  0.027296  ...  0.365344  0.198707  1.399693   \n",
       "2     0.895347  0.171569  0.398198  ...  0.066899  0.176659  0.359395   \n",
       "3     0.248858  1.179098  0.175501  ...  1.592119  0.726908  0.105380   \n",
       "4     0.134307  0.566226  0.000000  ...  0.197924  0.048724  0.188837   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4352  0.790157  0.523200  0.052590  ...  0.000000  0.032262  1.513051   \n",
       "4353  0.648776  0.284473  0.119037  ...  0.000000  0.048069  0.175962   \n",
       "4354  0.369393  0.303122  0.133160  ...  0.241998  0.342492  1.211094   \n",
       "4355  0.881679  0.000000  0.023721  ...  0.002158  0.121588  0.042624   \n",
       "4356  0.116978  0.041561  0.188517  ...  0.137291  0.067129  0.042314   \n",
       "\n",
       "          2041      2042      2043      2044      2045      2046      2047  \n",
       "0     0.503198  0.408284  0.370589  0.138303  1.269258  0.714142  0.119987  \n",
       "1     0.603393  1.222108  0.172571  0.954101  0.265650  0.282168  0.026532  \n",
       "2     0.377675  0.091454  0.402437  0.138679  0.673525  0.088165  0.081927  \n",
       "3     0.464521  0.435159  0.291601  0.380927  0.423609  0.114927  0.189419  \n",
       "4     0.223754  0.042908  0.075234  0.171219  0.348125  0.000000  0.295640  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4352  0.404489  1.358293  0.007750  1.634978  0.037231  0.249093  0.293531  \n",
       "4353  0.443320  0.247849  0.079690  0.621297  0.088509  0.536190  0.017956  \n",
       "4354  0.260014  0.319898  0.348717  0.205331  0.615079  0.037420  0.101339  \n",
       "4355  0.299990  0.657176  0.152649  0.199864  0.216885  0.111537  0.039340  \n",
       "4356  0.839931  0.048468  0.374847  0.817358  0.125033  0.029065  0.532317  \n",
       "\n",
       "[4357 rows x 2048 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Set: 0.9962917181705809\n",
      "Accuracy on Test Set: 0.7413357815010329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./svm_scaler1.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_A_scaled = scaler.fit_transform(X_A)\n",
    "\n",
    "# 分割原始数据集\n",
    "#X_A_train, X_A_test, Y_A_train, Y_A_test = train_test_split(X_A_scaled, Y_A, test_size=0.3, random_state=42)\n",
    "\n",
    "# 在原始多维数据上训练SVM分类器\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_A_scaled, Y_A)\n",
    "\n",
    "# 对测试数据进行相同的标准化\n",
    "X_R_scaled = scaler.transform(X_R)\n",
    "\n",
    "# 在训练集上的预测和准确率\n",
    "train_predictions = clf.predict(X_A_scaled)\n",
    "train_accuracy = accuracy_score(Y_A, train_predictions)\n",
    "print(\"Accuracy on Training Set:\", train_accuracy)\n",
    "\n",
    "# 在测试集上的预测和准确率\n",
    "test_predictions = clf.predict(X_R_scaled)\n",
    "test_accuracy = accuracy_score(Y_R, test_predictions)\n",
    "print(\"Accuracy on Test Set:\", test_accuracy)\n",
    "\n",
    "# 保存模型和标准化器\n",
    "joblib.dump(clf, './svm_model1.pkl')\n",
    "joblib.dump(scaler, './svm_scaler1.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.2839e+03 -1.1869e+07  1e+07  7e-18  3e-10\n",
      " 1:  1.2748e+03 -2.1906e+05  2e+05  8e-16  6e-12\n",
      " 2:  1.0805e+03 -4.2082e+03  5e+03  6e-16  4e-14\n",
      " 3:  8.1232e+02 -3.2520e+03  4e+03  5e-16  3e-14\n",
      " 4:  1.8278e+03 -2.4635e+03  4e+03  1e-15  4e-13\n",
      " 5:  8.1798e+02 -3.7519e+03  5e+03  4e-16  4e-13\n",
      " 6:  8.0535e+02  6.6800e+02  1e+02  5e-16  3e-14\n",
      " 7:  8.0277e+02  8.0116e+02  2e+00  2e-15  4e-15\n",
      " 8:  8.0275e+02  8.0273e+02  2e-02  5e-16  5e-14\n",
      " 9:  8.0275e+02  8.0275e+02  2e-04  2e-15  1e-14\n",
      "Optimal solution found.\n",
      "Accuracy on Training Set: 0.9962917181705809\n",
      "Accuracy on Test Set: 0.7296304796878587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./svm_scaler_kmm1.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import KMM\n",
    "\n",
    "beta = KMM.KMM(kernel_type='rbf', B=10).fit(X_A, X_R)\n",
    "X_A_kmm = beta * X_A\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_A_kmm_scaled = scaler.fit_transform(X_A_kmm)\n",
    "\n",
    "# 分割原始数据集\n",
    "#X_A_train, X_A_test, Y_A_train, Y_A_test = train_test_split(X_A_scaled, Y_A, test_size=0.3, random_state=42)\n",
    "\n",
    "# 在原始多维数据上训练SVM分类器\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_A_kmm_scaled, Y_A)\n",
    "\n",
    "# 对测试数据进行相同的标准化\n",
    "X_R_kmm_scaled = scaler.transform(X_R)\n",
    "\n",
    "# 在训练集上的预测和准确率\n",
    "train_predictions = clf.predict(X_A_kmm_scaled)\n",
    "train_accuracy = accuracy_score(Y_A, train_predictions)\n",
    "print(\"Accuracy on Training Set:\", train_accuracy)\n",
    "\n",
    "# 在测试集上的预测和准确率\n",
    "test_predictions = clf.predict(X_R_kmm_scaled)\n",
    "test_accuracy = accuracy_score(Y_R, test_predictions)\n",
    "print(\"Accuracy on Test Set:\", test_accuracy)\n",
    "\n",
    "# 保存模型和标准化器\n",
    "joblib.dump(clf, './svm_model_kmm1.pkl')\n",
    "joblib.dump(scaler, './svm_scaler_kmm1.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Set: 0.9818706221672847\n",
      "Accuracy on Test Set: 0.6974982786320862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./svm_scaler_tca1.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import TCA\n",
    "\n",
    "X_A_tca, X_R_tca = TCA.TCA(kernel_type='linear', dim=300, lamb=1, gamma=1).fit(X_A, X_R)\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_A_tca_scaled = scaler.fit_transform(X_A_tca)\n",
    "\n",
    "# 分割原始数据集\n",
    "#X_A_train, X_A_test, Y_A_train, Y_A_test = train_test_split(X_A_scaled, Y_A, test_size=0.3, random_state=42)\n",
    "\n",
    "# 在原始多维数据上训练SVM分类器\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_A_tca_scaled, Y_A)\n",
    "\n",
    "# 对测试数据进行相同的标准化\n",
    "X_R_tca_scaled = scaler.transform(X_R_tca)\n",
    "\n",
    "# 在训练集上的预测和准确率\n",
    "train_predictions = clf.predict(X_A_tca_scaled)\n",
    "train_accuracy = accuracy_score(Y_A, train_predictions)\n",
    "print(\"Accuracy on Training Set:\", train_accuracy)\n",
    "\n",
    "# 在测试集上的预测和准确率\n",
    "test_predictions = clf.predict(X_R_tca_scaled)\n",
    "test_accuracy = accuracy_score(Y_R, test_predictions)\n",
    "print(\"Accuracy on Test Set:\", test_accuracy)\n",
    "\n",
    "# 保存模型和标准化器\n",
    "joblib.dump(clf, './svm_model_tca1.pkl')\n",
    "joblib.dump(scaler, './svm_scaler_tca1.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Training Set: 0.9962917181705809\n",
      "Accuracy on Test Set: 0.7307780582969934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./svm_scaler_coral1.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import CORAL\n",
    "\n",
    "X_A_coral= CORAL.CORAL().fit(X_A, X_R)\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_A_coral_scaled = scaler.fit_transform(X_A_coral)\n",
    "\n",
    "# 分割原始数据集\n",
    "#X_A_train, X_A_test, Y_A_train, Y_A_test = train_test_split(X_A_scaled, Y_A, test_size=0.3, random_state=42)\n",
    "\n",
    "# 在原始多维数据上训练SVM分类器\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_A_coral_scaled, Y_A)\n",
    "\n",
    "# 对测试数据进行相同的标准化\n",
    "X_R_coral_scaled = scaler.transform(X_R)\n",
    "\n",
    "# 在训练集上的预测和准确率\n",
    "train_predictions = clf.predict(X_A_coral_scaled)\n",
    "train_accuracy = accuracy_score(Y_A, train_predictions)\n",
    "print(\"Accuracy on Training Set:\", train_accuracy)\n",
    "\n",
    "# 在测试集上的预测和准确率\n",
    "test_predictions = clf.predict(X_R_coral_scaled)\n",
    "test_accuracy = accuracy_score(Y_R, test_predictions)\n",
    "print(\"Accuracy on Test Set:\", test_accuracy)\n",
    "\n",
    "# 保存模型和标准化器\n",
    "joblib.dump(clf, './svm_model_coral1.pkl')\n",
    "joblib.dump(scaler, './svm_scaler_coral1.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StaticClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
